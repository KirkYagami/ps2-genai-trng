{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e49e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3dca119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents (pages) in pages_pdf: 3\n"
     ]
    }
   ],
   "source": [
    "# # Install LangChain and PyPDF if not already installed\n",
    "# !pip install langchain pypdf\n",
    "\n",
    "# Import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Path to your PDF file\n",
    "file_path = \"data/365_Data_Science_Courses.pdf\"\n",
    "\n",
    "# Create loader instance\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# Load the PDF\n",
    "pages_pdf = loader.load()\n",
    "\n",
    "# Check the length\n",
    "print(\"Number of documents (pages) in pages_pdf:\", len(pages_pdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3a7c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pages_md_split: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "# Step 1: Combine all PDF page contents into a single string\n",
    "pages_string = \"\"\n",
    "for page in pages_pdf:\n",
    "    pages_string += page.page_content + \"\\n\"\n",
    "\n",
    "# Step 2: Define headers for splitting\n",
    "headers = [\n",
    "    (\"#\", \"Course Title\"),\n",
    "    (\"##\", \"Lecture Title\")\n",
    "]\n",
    "\n",
    "# Step 3: Create MarkdownHeaderTextSplitter instance\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers)\n",
    "\n",
    "# Step 4: Apply the split_text() method\n",
    "pages_md_split = md_splitter.split_text(pages_string)\n",
    "\n",
    "# Step 5: Print the length\n",
    "print(\"Length of pages_md_split:\", len(pages_md_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0684b04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pages_char_split: 17\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Create the CharacterTextSplitter\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\". \",      # ensures we split on sentence boundaries\n",
    "    chunk_size=400,      # max length per chunk\n",
    "    chunk_overlap=0,     # no characters repeated across chunks\n",
    ")\n",
    "\n",
    "# Apply split_documents on pages_md_split\n",
    "pages_char_split = char_splitter.split_documents(pages_md_split)\n",
    "\n",
    "# Print length of resulting list\n",
    "print(\"Length of pages_char_split:\", len(pages_char_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce90523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs (if needed)\n",
    "# !pip install -q langchain langchain-openai numpy\n",
    "\n",
    "import os, numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# --- set your key ---\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY_HERE\"\n",
    "\n",
    "# embeddings model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# grab the two chunks' text\n",
    "t1 = pages_char_split[1].page_content\n",
    "t2 = pages_char_split[8].page_content\n",
    "\n",
    "# get vectors\n",
    "v1 = np.array(embedding.embed_query(t1), dtype=float)\n",
    "v2 = np.array(embedding.embed_query(t2), dtype=float)\n",
    "\n",
    "# cosine-style dot product (normalize first so dot==cosine similarity)\n",
    "v1n, v2n = v1/np.linalg.norm(v1), v2/np.linalg.norm(v2)\n",
    "sim = float(np.dot(v1n, v2n))\n",
    "\n",
    "print(\"Dot product:\", sim)\n",
    "print(\"Rounded to 2 decimals:\", round(sim, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c171daca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2c89cfbf904ac9abe003861a96b2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NikhilSharma\\.conda\\envs\\llms2\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NikhilSharma\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4f2555761a4f1ea331e343b31795e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56eecd966a744bfdb6ac4dabf05c4ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311a29a3d1bf4c9aab6c0140b1763055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93170c2c0a8142e39a3598a87dd64689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01ba1df00d4471a91dce24998cb47da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912187a495134370bbb878fbcf6f98b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42eebbe953b24cbfbd1ed8c27f2b3414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9210929b46df4e69b739faca91ed0390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f74d67581e4f22a6fc70141f082b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111beae35fa44c45bde8f6c76e521644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 0.2556152939796448\n",
      "Rounded: 0.26\n"
     ]
    }
   ],
   "source": [
    "# Install sentence-transformers if not already\n",
    "# !pip install -q sentence-transformers\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "t1 = pages_char_split[1].page_content\n",
    "t2 = pages_char_split[8].page_content\n",
    "\n",
    "# Encode\n",
    "v1 = model.encode(t1, convert_to_numpy=True)\n",
    "v2 = model.encode(t2, convert_to_numpy=True)\n",
    "\n",
    "# Normalize\n",
    "v1n, v2n = v1 / np.linalg.norm(v1), v2 / np.linalg.norm(v2)\n",
    "\n",
    "# Cosine similarity = dot product of normalized vectors\n",
    "sim = float(np.dot(v1n, v2n))\n",
    "\n",
    "print(\"Dot product:\", sim)\n",
    "print(\"Rounded:\", round(sim, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b44e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
