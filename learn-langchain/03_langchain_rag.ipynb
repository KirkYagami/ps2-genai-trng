{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6125cc32",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "## Overview\n",
    "\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "**Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happens offline._\n",
    "\n",
    "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:\n",
    "\n",
    "### Indexing\n",
    "\n",
    "1. **Load**: First we need to load our data. This is done with [Document Loaders](https://python.langchain.com/docs/concepts/document_loaders/).\n",
    "2. **Split**: [Text splitters](https://python.langchain.com/docs/concepts/text_splitters/) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) and [Embeddings](https://python.langchain.com/docs/concepts/embedding_models/) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e4df4",
   "metadata": {},
   "source": [
    "Side note:\n",
    "\n",
    "The context window (or “context length”) of a large language model (LLM) is the amount of text, in tokens, that the model can consider or “remember” at any one time. A larger context window enables an AI model to process longer inputs and incorporate a greater amount of information into each output.\n",
    "\n",
    "> https://codingscape.com/blog/llms-with-largest-context-windows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488b81e",
   "metadata": {},
   "source": [
    "![rag_indexing_phase](../assets/images/rag_indexing_phase.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65729616",
   "metadata": {},
   "source": [
    "### Retrieval and generation\n",
    "\n",
    "4. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://python.langchain.com/docs/concepts/retrievers/).\n",
    "5. **Generate**: A [ChatModel](https://python.langchain.com/docs/concepts/chat_models/) / [LLM](https://python.langchain.com/docs/concepts/text_llms/) produces an answer using a prompt that includes both the question with the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a7ef8",
   "metadata": {},
   "source": [
    "![](../assets/images/rag_phase_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92650ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a120074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fa4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8059e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"rag_langchain\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a78c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/LeaveNoContextBehind.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae880a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preprint. Under review.\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation. A key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention. The Infini-attention incorporates\\na compressive memory into the vanilla attention mechanism and builds\\nin both masked local attention and long-term linear attention mechanisms\\nin a single Transformer block. We demonstrate the effectiveness of our\\napproach on long-context language modeling benchmarks, 1M sequence\\nlength passkey context block retrieval and 500K length book summarization\\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\\nmemory parameters and enables fast streaming inference for LLMs.\\n1 Introduction\\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\\na constrained context-dependent memory, due to the nature of the attention mechanism.\\nUpdate\\nVV\\nConcat Concat\\nQ VQ VQs {KV}s\\nCompressive memory & \\nLinear attention\\nCausal scaled dot-product \\nattention & PE\\nLinear \\nprojection\\n{KV}s-1\\nRetrieve\\nFigure 1: Infini-attention has an addi-\\ntional compressive memory with linear\\nattention for processing infinitely long\\ncontexts. {KV}s−1 and {KV}s are atten-\\ntion key and values for current and previ-\\nous input segments, respectively and Qs\\nthe attention queries. PE denotes position\\nembeddings.\\nThe attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory\\nfootprint and computation time. For example,\\nthe attention Key-Value (KV) states have 3TB\\nmemory footprint for a 500B model with batch\\nsize 512 and context length 2048 (Pope et al.,\\n2023). Indeed, scaling LLMs to longer sequences\\n(i.e. 1M tokens) is challenging with the standard\\nTransformer architectures and serving longer\\nand longer context models becomes costly finan-\\ncially.\\nCompressive memory systems promise to be\\nmore scalable and efficient than the attention\\nmechanism for extremely long sequences (Kan-\\nerva, 1988; Munkhdalai et al., 2019). Instead\\nof using an array that grows with the input se-\\nquence length, a compressive memory primarily\\nmaintains a fixed number of parameters to store\\nand recall information with a bounded storage\\nand computation costs. In the compressive mem-\\nory, new information is added to the memory\\nby changing its parameters with an objective\\nthat this information can be recovered back later\\non. However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n1\\narXiv:2404.07143v1  [cs.CL]  10 Apr 2024'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7ec325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total pages\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aeb590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "711f43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the document into chunks\n",
    "\n",
    "# from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "# text_splitter = NLTKTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "# print(len(chunks))\n",
    "\n",
    "# print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106a7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17144e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0060ab1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NikhilSharma\\.conda\\envs\\llms2\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "275ee017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a Connection with the ChromaDB\n",
    "db_connection = Chroma(collection_name=\"rag_langchain\",persist_directory=\"./chroma_langchain_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54890047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "# Converting CHROMA db_connection to Retriever Object\n",
    "retriever = db_connection.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(type(retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Leave No Context Behind\" introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. This is achieved through a new attention technique called Infini-attention, which incorporates a compressive memory, masked local attention, and long-term linear attention into a single Transformer block. The approach demonstrates effectiveness on long-context benchmarks, including 1M sequence length passkey retrieval and 500K length book summarization, enabling fast streaming inference.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"\"\"Please summarize Leave No Context Behind:\n",
    "                            Efficient Infinite Context Transformers with Infinite-attention\"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4469593e",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10873090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # System Message Prompt Template\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot.\n",
    "                  Given a context and question from user,\n",
    "                  you should answer based on the given context.\"\"\"),\n",
    "    # Human Message Prompt Template\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer: \"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e216d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b8bcdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\"Leave No Context Behind\" introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. This is achieved through a new attention technique called Infini-attention, which incorporates a compressive memory, masked local attention, and long-term linear attention into a single Transformer block. The approach demonstrates effectiveness on long-context benchmarks, including 1M sequence length passkey retrieval and 500K length book summarization, enabling fast streaming inference."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd0797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Compressive memory systems maintain a constant number of memory parameters, unlike Transformer KV memory which grows with input sequence length, to ensure computational efficiency and bounded storage. They store and retrieve information by modifying these fixed parameters via an update rule and a memory reading mechanism. This approach aims for greater scalability and efficiency for processing extremely long sequences compared to standard attention mechanisms.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"\"\"Please Explain Compressive Memory\"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "babf96bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Compressive memory systems maintain a constant number of memory parameters, unlike Transformer KV memory which grows with input sequence length, to ensure computational efficiency and bounded storage. They store and retrieve information by modifying these fixed parameters via an update rule and a memory reading mechanism. This approach aims for greater scalability and efficiency for processing extremely long sequences compared to standard attention mechanisms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c3201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
